# RAG System Environment Variables
# Copy this file to .env and fill in your actual values

# =============================================================================
# HUGGING FACE CONFIGURATION
# =============================================================================

# Your HuggingFace access token (required for downloading models)
# Get your token from: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=your_huggingface_token_here

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Model name to download and use
MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct

# Path to the converted OpenVINO GenAI model
MODEL_PATH=./models/Llama-3.1-8B-Instruct-int4-genai/original_model

# =============================================================================
# VECTOR STORE CONFIGURATION
# =============================================================================

# Path to the vector store directory
VECTOR_STORE_PATH=./data/processed_data/vector_store

# Embedding model for vector search
EMBEDDING_MODEL=all-MiniLM-L6-v2

# =============================================================================
# INFERENCE CONFIGURATION
# =============================================================================

# Device to run inference on (CPU/GPU/AUTO)
DEVICE=AUTO

# Number of chunks to retrieve for context
K_CHUNKS=5

# Maximum tokens to generate in response
MAX_TOKENS=512

# Sampling temperature for response generation
TEMPERATURE=0.7

# =============================================================================
# EXAMPLE USAGE
# =============================================================================

# To use this file:
# 1. Copy this file to .env: cp .env.sample .env
# 2. Edit .env and fill in your actual values
# 3. Make sure to set your HuggingFace token
# 4. Run the RAG system: python rag_cli.py setup

# Example commands:
# python rag_cli.py setup
# python rag_cli.py convert-model
# python rag_cli.py --query "What is Procyon?"
# python rag_cli.py hardware --detailed
# python rag_cli.py performance --benchmark

# =============================================================================
# DOCUMENT PROCESSING CONFIGURATION
# =============================================================================

# Path to the source PDF document
PDF_PATH=./data/raw/procyon_guide.pdf

# Chunk size for document processing (in characters)
CHUNK_SIZE=512

# Chunk overlap for document processing (in characters)
CHUNK_OVERLAP=50

# =============================================================================
# PERFORMANCE CONFIGURATION
# =============================================================================

# Enable performance monitoring
ENABLE_MONITORING=true

# Performance monitoring duration (seconds)
MONITORING_DURATION=60
